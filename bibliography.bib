@article{bottou2018optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	journal={SIAM review},
	volume={60},
	number={2},
	pages={223--311},
	year={2018},
	publisher={SIAM}
}

@article{berthier2025diagonal,
  title={Diagonal Linear Networks and the Lasso Regularization Path},
  author={Berthier, Rapha{\"e}l},
  journal={arXiv preprint arXiv:2509.18766},
  year={2025}
}


@inproceedings{dragomir2021fast,
  title={Fast stochastic bregman gradient methods: Sharp analysis and variance reduction},
  author={Dragomir, Radu Alexandru and Even, Mathieu and Hendrikx, Hadrien},
  booktitle={International Conference on Machine Learning},
  pages={2815--2825},
  year={2021},
  organization={PMLR}
}


@article{attouch1996viscosity,
  title={Viscosity solutions of minimization problems},
  author={Attouch, Hedy},
  journal={SIAM Journal on Optimization},
  volume={6},
  number={3},
  pages={769--806},
  year={1996},
  publisher={SIAM}
}


@article{berthier2023incremental,
  title={Incremental learning in diagonal linear networks},
  author={Berthier, Rapha{\"e}l},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={171},
  pages={1--26},
  year={2023}
}


@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}


@article{even2023s,
  title={(S) GD over Diagonal Linear Networks: Implicit bias, Large Stepsizes and Edge of Stability},
  author={Even, Mathieu and Pesme, Scott and Gunasekar, Suriya and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={29406--29448},
  year={2023}
}


@article{poon2023smooth,
  title={Smooth over-parameterized solvers for non-smooth structured optimization},
  author={Poon, Clarice and Peyr{\'e}, Gabriel},
  journal={Mathematical programming},
  volume={201},
  number={1},
  pages={897--952},
  year={2023},
  publisher={Springer}
}


@article{tibshirani2021equivalences,
  title={Equivalences between sparse models and neural networks},
  author={Tibshirani, Ryan J},
  journal={Working Notes. URL https://www. stat. cmu. edu/ryantibs/papers/sparsitynn. pdf},
  year={2021}
}

@book{bhatia2013matrix,
  title={Matrix analysis},
  author={Bhatia, Rajendra},
  volume={169},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas and Schmidt, Mark and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  pages={83--112},
  year={2017},
  publisher={Springer}
}


@article{needell2014stochastic,
  title={Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm},
  author={Needell, Deanna and Ward, Rachel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{montanari2012graphical,
	title={Graphical models concepts in compressed sensing.},
	author={Montanari, Andrea},
	journal={Compressed Sensing},
	pages={394--438},
	year={2012}
}


@phdthesis{berthier2021analysis,
	title={Analysis and acceleration of gradient descents and gossip algorithms},
	author={Berthier, Rapha{\"e}l},
	year={2021},
	school={Universit{\'e} Paris sciences et lettres}
}

@article{even2021continuized,
	title={A continuized view on nesterov acceleration for stochastic gradient descent and randomized gossip},
	author={Even, Mathieu and Berthier, Rapha{\"e}l and Bach, Francis and Flammarion, Nicolas and Gaillard, Pierre and Hendrikx, Hadrien and Massouli{\'e}, Laurent and Taylor, Adrien},
	journal={arXiv preprint arXiv:2106.07644},
	year={2021}
}


@article{daspremont2021acceleration,
	title={Acceleration methods},
	author={dâ€™Aspremont, Alexandre and Scieur, Damien and Taylor, Adrien and others},
	journal={Foundations and Trends{\textregistered} in Optimization},
	volume={5},
	number={1-2},
	pages={1--245},
	year={2021},
	publisher={Now Publishers, Inc.}
}


@misc{bach2024learning,
	author       = {Francis Bach},
	title        = {Learning theory from first principles},
	howpublished = {Lecture notes, available online at \url{https://www.di.ens.fr/~fbach/ltfp_book.pdf}},
	note         = {Accessed: Oct. 4, 2024},
	year         = {2024},
}


@misc{bolthausen,
	author       = {Erwin Bolthausen},
	title        = {The Thouless-Anderson-Palmer equation in spin
	glass theory},
	howpublished = {Lecture notes, available online at \url{https://anr-malin.sciencesconf.org/data/pages/Aussois_2.pdf}},
	note         = {Accessed: Oct. 4, 2024},
}

@misc{royer2024,
	author       = {Cl\'ement Royer},
	title        = {Optimization for machine learning},
	howpublished = {Lecture notes, available online at \url{https://www.lamsade.dauphine.fr/%7Ecroyer/ensdocs/OID/PolyOID.pdf}},
	note         = {Accessed: Oct. 4, 2024},
	year={2024}
}

@misc{duchi2016,
	author       = {John Duchi},
	title        = {Introductory lectures on stochastic convex optimization},
	howpublished = {Lecture notes, available online at \url{https://stanford.edu/~jduchi/PCMIConvex/}},
	note         = {Accessed: Oct. 21, 2024},
	year={2016}
}

@article{bartlett2021deep,
	title={Deep learning: a statistical viewpoint},
	author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
	journal={Acta numerica},
	volume={30},
	pages={87--201},
	year={2021},
	publisher={Cambridge University Press}
}
