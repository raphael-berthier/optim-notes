\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimization in machine learning training.}{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Peculiarities of optimization in machine learning.}{3}{section*.4}\protected@file@percent }
\citation{bach2024learning}
\@writefile{toc}{\contentsline {paragraph}{Goal of this course.}{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Empirical risk minimization and generalization error}{4}{section.2}\protected@file@percent }
\newlabel{sec:empirical-risk}{{2}{4}{Empirical risk minimization and generalization error}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Formal expression of a learning problem}{4}{subsection.2.1}\protected@file@percent }
\newlabel{rmk:convexification}{{2.1}{5}{}{assumption.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Decomposition of the error}{6}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Decomposition between estimation and approximation errors}{6}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{eq:estimation-error}{{2.1}{6}{Decomposition between estimation and approximation errors}{equation.2.1}{}}
\citation{bach2024learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Optimization error}{7}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{eq:decomposition-1}{{2.2}{7}{Optimization error}{equation.2.2}{}}
\newlabel{eq:decomposition-2}{{2.3}{7}{Optimization error}{equation.2.3}{}}
\newlabel{eq:decomposition-4}{{2.4}{7}{Optimization error}{equation.2.4}{}}
\newlabel{eq:decomposition-3}{{2.5}{7}{Optimization error}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Relevance of this decomposition and goal of the course}{7}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Take-home message for optimizers.}{8}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Stochastic gradient descent}{8}{section.3}\protected@file@percent }
\newlabel{sec:sgd}{{3}{8}{Stochastic gradient descent}{section.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Notations.}{9}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient descent on the empirical risk}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:gd}{{3.1}{9}{Gradient descent on the empirical risk}{subsection.3.1}{}}
\newlabel{eq:gd}{{3.1}{9}{Gradient descent on the empirical risk}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Multi-pass stochastic gradient descent on the empirical risk}{9}{subsection.3.2}\protected@file@percent }
\newlabel{sec:sgd-empirical}{{3.2}{9}{Multi-pass stochastic gradient descent on the empirical risk}{subsection.3.2}{}}
\newlabel{eq:sgd-empirical-mini-batch}{{3.2}{9}{Multi-pass stochastic gradient descent on the empirical risk}{equation.3.2}{}}
\newlabel{eq:sgd-empirical}{{3.3}{10}{Multi-pass stochastic gradient descent on the empirical risk}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Abstract stochastic gradient descent}{10}{subsection.3.3}\protected@file@percent }
\newlabel{sec:sgd-abstract}{{3.3}{10}{Abstract stochastic gradient descent}{subsection.3.3}{}}
\newlabel{eq:sgd-abstract}{{3.4}{10}{Abstract stochastic gradient descent}{equation.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Example: multi-pass stochastic gradient descent.}{10}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{11}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Single-pass stochastic gradient descent on the expected risk }{11}{subsection.3.4}\protected@file@percent }
\newlabel{sec:sgd-expected}{{3.4}{11}{Single-pass stochastic gradient descent on the expected risk}{subsection.3.4}{}}
\newlabel{eq:sgd-expected}{{3.5}{11}{Single-pass stochastic gradient descent on the expected risk}{equation.3.5}{}}
\newlabel{eq:stochastic-approximation}{{3.6}{12}{stochastic approximation}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Coordinate gradient descent}{12}{subsection.3.5}\protected@file@percent }
\newlabel{sec:cgd}{{3.5}{12}{Coordinate gradient descent}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Exercises}{13}{subsection.3.6}\protected@file@percent }
\newlabel{exer:gossip}{{3.3}{13}{gossip problem}{assumption.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analyses for stochastic gradient descent}{14}{section.4}\protected@file@percent }
\newlabel{sec:analysis}{{4}{14}{Analyses for stochastic gradient descent}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Function structures}{14}{subsection.4.1}\protected@file@percent }
\newlabel{sec:function-structures}{{4.1}{14}{Function structures}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Convexity}{14}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{prop:convex-diff}{{4.2}{14}{}{assumption.4.2}{}}
\newlabel{prop:convex-2diff}{{4.4}{15}{}{assumption.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Strong convexity}{16}{subsubsection.4.1.2}\protected@file@percent }
\newlabel{prop:strong-convex-gen}{{4.6}{16}{}{assumption.4.6}{}}
\newlabel{prop:strong-convex-diff}{{4.7}{17}{}{assumption.4.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Smoothness}{18}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{prop:smooth-gen}{{4.11}{18}{}{assumption.4.11}{}}
\newlabel{prop:smooth-diff}{{4.12}{19}{}{assumption.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Analysis of gradient descent}{20}{subsection.4.2}\protected@file@percent }
\newlabel{sec:analysis-gd}{{4.2}{20}{Analysis of gradient descent}{subsection.4.2}{}}
\newlabel{thm:gd}{{4.14}{20}{}{assumption.4.14}{}}
\newlabel{coro:gd}{{4.16}{21}{}{assumption.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Analysis of abstract stochastic gradient descent}{22}{subsection.4.3}\protected@file@percent }
\newlabel{sec:analysis-sgd}{{4.3}{22}{Analysis of abstract stochastic gradient descent}{subsection.4.3}{}}
\newlabel{assu:strong-convexity}{{4.17}{22}{}{assumption.4.17}{}}
\newlabel{assu:cocoercivity-grad-sto}{{4.18}{22}{}{assumption.4.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic approximation setting.}{22}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coordinate gradient descent.}{23}{section*.11}\protected@file@percent }
\newlabel{thm:sgd}{{4.19}{23}{}{assumption.4.19}{}}
\newlabel{eq:sgd-bound}{{4.1}{23}{}{equation.4.1}{}}
\newlabel{coro:sgd}{{4.20}{25}{}{assumption.4.20}{}}
\newlabel{eq:aux1}{{4.2}{25}{Coordinate gradient descent}{equation.4.2}{}}
\newlabel{thm:sgd-decay}{{4.21}{26}{}{assumption.4.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Application for multi-pass stochastic gradient descent on the empirical risk}{27}{subsection.4.4}\protected@file@percent }
\newlabel{sec:analysis-sgd-empirical}{{4.4}{27}{Application for multi-pass stochastic gradient descent on the empirical risk}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Batch-size $m=1$.}{27}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General batch-size $m \geqslant 1$.}{28}{section*.13}\protected@file@percent }
\newlabel{eq:train-minibatch-precision}{{4.3}{29}{General batch-size $m \geq 1$}{equation.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Application for single-pass stochastic gradient descent on the expected risk}{30}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Application for coordinate gradient descent}{31}{subsection.4.6}\protected@file@percent }
\newlabel{sec:application-cgd}{{4.6}{31}{Application for coordinate gradient descent}{subsection.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Exercises}{32}{subsection.4.7}\protected@file@percent }
\newlabel{exer:improved-smoothness-batches}{{4.23}{32}{}{assumption.4.23}{}}
\newlabel{exer:sgd-optimality}{{4.25}{33}{tightness and statistical optimality}{assumption.4.25}{}}
\citation{needell2014stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {5}Exercise/practical session: importance sampling}{34}{section.5}\protected@file@percent }
\newlabel{sec:importance-sampling}{{5}{34}{Exercise/practical session: importance sampling}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Stochastic gradient descent for finite sums}{34}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Simulations: the least-squares case}{35}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Simulations of question 8. We observe that in case (a) (upper plots), as all functions have smoothness constants with the same order of magnitude, all algorithms behave similarly. In case (b), as one function has a much larger smoothness constant, differences appear. In the left plot, we observe that the two biased algorithms allow to take much larger stepsizes and thus obtain a much better initial decrease of the distance to the optimum. This is due to an improvement in the conditioning $M/\mu $ of the problem. However, they stabilize at a much larger error. This is due, of course, to the fact that the stepsize is larger. However, as the right plot shows, even with equal stepsizes, the biased algorithm stabilizes at larger errors (and in this case, the initial decrease is the same). This effect is mitigated in the case of the partially biased algorithm.}}{36}{figure.caption.14}\protected@file@percent }
\citation{defazio2014saga}
\citation{johnson2013accelerating}
\citation{roux2012stochastic}
\citation{schmidt2017minimizing}
\@writefile{toc}{\contentsline {section}{\numberline {6}Variance reduction by gradient aggregation}{37}{section.6}\protected@file@percent }
\newlabel{sec:variance-reduction}{{6}{37}{Variance reduction by gradient aggregation}{section.6}{}}
\newlabel{eq:aux2}{{6.1}{39}{Variance reduction by gradient aggregation}{equation.6.1}{}}
\newlabel{eq:aux3}{{6.2}{40}{Variance reduction by gradient aggregation}{equation.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Neural networks and sparse regularization}{40}{section.7}\protected@file@percent }
\newlabel{sec:neural}{{7}{40}{Neural networks and sparse regularization}{section.7}{}}
\citation{tibshirani2021equivalences}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Neural networks with weight decay}{41}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Diagonal linear networks}{41}{subsubsection.7.1.1}\protected@file@percent }
\newlabel{sec:equivalences-dln}{{7.1.1}{41}{Diagonal linear networks}{subsubsection.7.1.1}{}}
\newlabel{eq:lasso}{{7.1}{41}{Diagonal linear networks}{equation.7.1}{}}
\newlabel{lem:dln}{{7.1}{41}{}{assumption.7.1}{}}
\newlabel{eq:dln}{{7.2}{42}{Diagonal linear networks}{equation.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Deeper diagonal linear networks}{43}{subsubsection.7.1.2}\protected@file@percent }
\newlabel{eq:deep-dln}{{7.3}{43}{Deeper diagonal linear networks}{equation.7.3}{}}
\newlabel{lem:deep-dln}{{7.2}{43}{}{assumption.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}Linear networks}{44}{subsubsection.7.1.3}\protected@file@percent }
\newlabel{sec:linear-networks}{{7.1.3}{44}{Linear networks}{subsubsection.7.1.3}{}}
\citation{bhatia2013matrix}
\newlabel{eq:linear}{{7.4}{45}{Linear networks}{equation.7.4}{}}
\newlabel{lem:linear}{{7.3}{45}{}{assumption.7.3}{}}
\newlabel{rmk:bhatia}{{7.4}{45}{}{assumption.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.4}Group diagonal linear networks}{46}{subsubsection.7.1.4}\protected@file@percent }
\newlabel{eq:group}{{7.5}{46}{Group diagonal linear networks}{equation.7.5}{}}
\newlabel{eq:group-lasso}{{7.6}{47}{Group diagonal linear networks}{equation.7.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.5}ReLU neural networks}{47}{subsubsection.7.1.5}\protected@file@percent }
\citation{bach2024learning}
\newlabel{eq:relu}{{7.7}{48}{ReLU neural networks}{equation.7.7}{}}
\citation{poon2023smooth}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Implicit regularization}{49}{subsection.7.2}\protected@file@percent }
\citation{even2023s}
\@writefile{toc}{\contentsline {paragraph}{Notation.}{50}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Linear parametrization}{50}{subsubsection.7.2.1}\protected@file@percent }
\newlabel{eq:gf-linear}{{7.8}{50}{Linear parametrization}{equation.7.8}{}}
\newlabel{eq:gd-linear}{{7.9}{50}{Linear parametrization}{equation.7.9}{}}
\newlabel{prop:implicit-reg-linear}{{7.9}{50}{}{assumption.7.9}{}}
\newlabel{eq:implicit-reg-ridge}{{7.10}{50}{Linear parametrization}{equation.7.10}{}}
\newlabel{eq:decrease-squared-distance}{{7.11}{51}{Linear parametrization}{equation.7.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Mirror parametrization}{52}{subsubsection.7.2.2}\protected@file@percent }
\newlabel{eq:mirror-flow}{{7.12}{53}{Mirror parametrization}{equation.7.12}{}}
\newlabel{prop:implicit-reg-mirror}{{7.10}{53}{}{assumption.7.10}{}}
\newlabel{eq:decrease-bregman}{{7.13}{54}{Mirror parametrization}{equation.7.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Diagonal linear networks}{55}{subsubsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The $u \circ u$ parametrization.}{55}{section*.16}\protected@file@percent }
\newlabel{eq:gradient-flow-dln}{{7.14}{55}{The $u \circ u$ parametrization}{equation.7.14}{}}
\newlabel{eq:mirror-flow-dln}{{7.15}{55}{The $u \circ u$ parametrization}{equation.7.15}{}}
\citation{attouch1996viscosity}
\newlabel{coro:implicit-reg-dln}{{7.11}{56}{}{assumption.7.11}{}}
\newlabel{eq:min-l1-u2}{{7.16}{57}{The $u \circ u$ parametrization}{equation.7.16}{}}
\@writefile{toc}{\contentsline {paragraph}{The $u \circ v$ parametrization.}{57}{section*.17}\protected@file@percent }
\newlabel{eq:gradient-flow-dln-uv}{{7.17}{57}{The $u \circ v$ parametrization}{equation.7.17}{}}
\newlabel{eq:gradient-flow-dln-wz}{{7.18}{58}{The $u \circ v$ parametrization}{equation.7.18}{}}
\citation{berthier2023incremental}
\citation{berthier2025diagonal}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Incremental learning}{59}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Exercises}{59}{subsection.7.4}\protected@file@percent }
\newlabel{exer:linear-networks}{{7.13}{59}{}{assumption.7.13}{}}
\newlabel{eq:lemma-linear-gen}{{7.19}{59}{Exercises}{equation.7.19}{}}
\bibstyle{apalike}
\bibdata{bibliography}
\bibcite{attouch1996viscosity}{Attouch, 1996}
\bibcite{bach2024learning}{Bach, 2024}
\bibcite{berthier2023incremental}{Berthier, 2023}
\bibcite{berthier2025diagonal}{Berthier, 2025}
\bibcite{bhatia2013matrix}{Bhatia, 2013}
\bibcite{defazio2014saga}{Defazio et~al., 2014}
\bibcite{even2023s}{Even et~al., 2023}
\bibcite{johnson2013accelerating}{Johnson and Zhang, 2013}
\bibcite{needell2014stochastic}{Needell et~al., 2014}
\bibcite{poon2023smooth}{Poon and Peyr{\'e}, 2023}
\bibcite{roux2012stochastic}{Roux et~al., 2012}
\bibcite{schmidt2017minimizing}{Schmidt et~al., 2017}
\bibcite{tibshirani2021equivalences}{Tibshirani, 2021}
\gdef \@abspage@last{62}
