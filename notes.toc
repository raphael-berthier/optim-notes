\contentsline {section}{\numberline {1}Introduction}{3}{section.1}%
\contentsline {paragraph}{Optimization in machine learning training.}{3}{section*.3}%
\contentsline {paragraph}{Peculiarities of optimization in machine learning.}{3}{section*.4}%
\contentsline {paragraph}{Goal of this course.}{4}{section*.5}%
\contentsline {section}{\numberline {2}Empirical risk minimization and generalization error}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}Formal expression of a learning problem}{4}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Decomposition of the error}{6}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Decomposition between estimation and approximation errors}{6}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Optimization error}{7}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Relevance of this decomposition and goal of the course}{7}{subsubsection.2.2.3}%
\contentsline {paragraph}{Take-home message for optimizers.}{8}{section*.6}%
\contentsline {section}{\numberline {3}Stochastic gradient descent}{8}{section.3}%
\contentsline {paragraph}{Notations.}{9}{section*.7}%
\contentsline {subsection}{\numberline {3.1}Gradient descent on the empirical risk}{9}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Multi-pass stochastic gradient descent on the empirical risk}{9}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Abstract stochastic gradient descent}{10}{subsection.3.3}%
\contentsline {paragraph}{Example: multi-pass stochastic gradient descent.}{10}{section*.8}%
\contentsline {paragraph}{}{11}{section*.9}%
\contentsline {subsection}{\numberline {3.4}Single-pass stochastic gradient descent on the expected risk }{11}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Coordinate gradient descent}{12}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Exercises}{13}{subsection.3.6}%
\contentsline {section}{\numberline {4}Analyses for stochastic gradient descent}{14}{section.4}%
\contentsline {subsection}{\numberline {4.1}Function structures}{14}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Convexity}{14}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Strong convexity}{16}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Smoothness}{18}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Analysis of gradient descent}{20}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Analysis of abstract stochastic gradient descent}{22}{subsection.4.3}%
\contentsline {paragraph}{Stochastic approximation setting.}{22}{section*.10}%
\contentsline {paragraph}{Coordinate gradient descent.}{23}{section*.11}%
\contentsline {subsection}{\numberline {4.4}Application for multi-pass stochastic gradient descent on the empirical risk}{27}{subsection.4.4}%
\contentsline {paragraph}{Batch-size $m=1$.}{27}{section*.12}%
\contentsline {paragraph}{General batch-size $m \geqslant 1$.}{28}{section*.13}%
\contentsline {subsection}{\numberline {4.5}Application for single-pass stochastic gradient descent on the expected risk}{30}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Application for coordinate gradient descent}{31}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Exercises}{32}{subsection.4.7}%
\contentsline {section}{\numberline {5}Exercise/practical session: importance sampling}{34}{section.5}%
\contentsline {subsection}{\numberline {5.1}Stochastic gradient descent for finite sums}{34}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Simulations: the least-squares case}{35}{subsection.5.2}%
\contentsline {section}{\numberline {6}Variance reduction by gradient aggregation}{37}{section.6}%
\contentsline {section}{\numberline {7}Neural networks and sparse regularization}{40}{section.7}%
\contentsline {subsection}{\numberline {7.1}Neural networks with weight decay}{41}{subsection.7.1}%
\contentsline {subsubsection}{\numberline {7.1.1}Diagonal linear networks}{41}{subsubsection.7.1.1}%
\contentsline {subsubsection}{\numberline {7.1.2}Deeper diagonal linear networks}{43}{subsubsection.7.1.2}%
\contentsline {subsubsection}{\numberline {7.1.3}Linear networks}{44}{subsubsection.7.1.3}%
\contentsline {subsubsection}{\numberline {7.1.4}Group diagonal linear networks}{46}{subsubsection.7.1.4}%
\contentsline {subsubsection}{\numberline {7.1.5}ReLU neural networks}{47}{subsubsection.7.1.5}%
\contentsline {subsection}{\numberline {7.2}Implicit regularization}{49}{subsection.7.2}%
\contentsline {paragraph}{Notation.}{50}{section*.15}%
\contentsline {subsubsection}{\numberline {7.2.1}Linear parametrization}{50}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Mirror parametrization}{52}{subsubsection.7.2.2}%
\contentsline {subsubsection}{\numberline {7.2.3}Diagonal linear networks}{55}{subsubsection.7.2.3}%
\contentsline {paragraph}{The $u \circ u$ parametrization.}{55}{section*.16}%
\contentsline {paragraph}{The $u \circ v$ parametrization.}{57}{section*.17}%
\contentsline {subsection}{\numberline {7.3}Incremental learning}{59}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Exercises}{59}{subsection.7.4}%
